{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from process_data import TextPreprocess\n",
    "\n",
    "\n",
    "radonpy_data = pd.read_csv('./radonpy.csv')\n",
    "# Remove the space of column name\n",
    "radonpy_data.columns = radonpy_data.columns.str.replace(' ', '')\n",
    "\n",
    "smiles_list = radonpy_data['smiles'].tolist()\n",
    "\n",
    "smiles_process = TextPreprocess()\n",
    "word2index, index2word, word_count = smiles_process.create_vocabulary(smiles_list)\n",
    "\n",
    "\n",
    "x = smiles_process.text_to_index(smiles_list, padding=True)\n",
    "x = torch.tensor(x, dtype=torch.int64)  # Convert to torch.long data type\n",
    "# Only use the first 10 samples\n",
    "x = x[:120]\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2]) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "class test_mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test_mlp, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 3)\n",
    "        self.fc2 = nn.Linear(3, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "test_x = torch.randn(5, 5)\n",
    "mlp = test_mlp()\n",
    "y = mlp(test_x)\n",
    "pred = y.argmax(dim=1)\n",
    "print(y.shape, pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, word_count, d_model, latent_dim, nhead=1):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(word_count, d_model)\n",
    "        \n",
    "        # Transformer encoder layer\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers for mean and log variance of latent space\n",
    "        self.fc_mu = nn.Sequential(\n",
    "            nn.Linear(d_model, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_logvar = nn.Sequential(\n",
    "            nn.Linear(d_model, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def reparameter(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick: sample from N(mu, sigma^2)\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)  # Compute standard deviation\n",
    "        eps = torch.randn_like(std)    # Sample epsilon from standard normal\n",
    "        return mu + eps * std          # Return reparameterized z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, seq_length)\n",
    "        \n",
    "        # Embedding the input sequence of tokens\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_length, d_model)\n",
    "        \n",
    "        # Pass through Transformer encoder\n",
    "        x = self.encoder(x)    # Shape: (batch_size, seq_length, d_model)\n",
    "        \n",
    "        # Apply mean pooling over the sequence length dimension\n",
    "        # Compress sequence dimension to get sentence representation\n",
    "        x = torch.mean(x, dim=1)  # Shape: (batch_size, d_model)\n",
    "        \n",
    "        # Compute mean and log variance for the latent distribution\n",
    "        mu = self.fc_mu(x)        # Shape: (batch_size, latent_dim)\n",
    "        logvar = self.fc_logvar(x)  # Shape: (batch_size, latent_dim)\n",
    "        \n",
    "        # Reparameterization to sample z from the latent space\n",
    "        z = self.reparameter(mu, logvar)  # Shape: (batch_size, latent_dim)\n",
    "        \n",
    "        return z, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 32]), torch.Size([10, 32]), torch.Size([10, 32]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = VAE_Encoder(word_count=39, d_model=64, latent_dim=32)\n",
    "x = torch.randint(0, 39, (10, 12), dtype=torch.int64)\n",
    "z, mu, logvar = encoder(x)\n",
    "z.shape, mu.shape, logvar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, word_count, latent_dim, d_model, hidden_dim ,output_dim, num_layers=1):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "        self.word_count = word_count\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_model = d_model\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(word_count, d_model)\n",
    "        self.decoder = nn.GRU(input_size=d_model, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        hidden = self.linear_transform(x, hidden)\n",
    "        x, hidden = self.decoder(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x, hidden\n",
    "\n",
    "    def get_linear_layer(self, number_layer=1, **kwargs):\n",
    "        layer_dims = kwargs.get('layer_dims', None)\n",
    "\n",
    "        if layer_dims is None:\n",
    "            raise ValueError(\"Layer dimensions must be provided\")\n",
    "        \n",
    "        if len(layer_dims) >3:\n",
    "            raise ValueError(\"Only 3 layer are supported\")\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(layer_dims)-1):\n",
    "            layer = nn.Linear(layer_dims[i], layer_dims[i+1])\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def linear_transform(self, x, hidden):\n",
    "        \"\"\"\n",
    "        The hidden size should be (num_layers, batch_size, hidden_dim).\n",
    "        The x size should be (batch_size, seq_len, d_model).\n",
    "        For the first time step, the x should be the <start> token.\n",
    "\n",
    "        x = torch.randn(10, 100, 64)        # Batch size 10, sequence length 12, input size 64\n",
    "        hidden = torch.randn(1, 10, 128)    # num_layers 1, batch size 10, hidden size 128\n",
    "        \"\"\"\n",
    "        \n",
    "        if hidden.shape[1] == x.shape[0] and hidden.shape[2] == self.hidden_dim:\n",
    "            print('The hidden satisfies the requirement')\n",
    "            return hidden\n",
    "\n",
    "\n",
    "        # If latent dimension is not the same as embedding dimension, we need to do the linear transformation\n",
    "        if hidden.shape[2] != self.hidden_dim:\n",
    "            print('Warning: The latent dimension is not the same as the hidden dimension, we do the linear transformation')\n",
    "            new_linear = self.get_linear_layer(layer_dims=[self.latent_dim, self.hidden_dim])\n",
    "            hidden = new_linear(hidden)\n",
    "            print('After linear transformation, the hidden shape is: ', hidden.shape)\n",
    "        \n",
    "\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The latent dimension is not the same as the hidden dimension, we do the linear transformation\n",
      "After linear transformation, the hidden shape is:  torch.Size([1, 10, 128])\n",
      "torch.Size([10, 12, 39]) torch.Size([1, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "decoder = VAE_Decoder(latent_dim=32, word_count=39, d_model=64, hidden_dim=128, output_dim=39, num_layers=1)\n",
    "trg_input = torch.randint(0, 39, (10, 12), dtype=torch.int64)\n",
    "hidden = z.unsqueeze(0)\n",
    "\n",
    "decoder_out, hidden = decoder(trg_input, hidden)\n",
    "print(decoder_out.shape, hidden.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, \n",
    "                src: torch.Tensor,\n",
    "                trg: torch.Tensor,\n",
    "                teacher_forcing: bool = False,\n",
    "                teacher_forcing_ratio: float = 0.5):\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        seq_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        output_seq = torch.zeros(batch_size, seq_len, trg_vocab_size).to(src.device)\n",
    "\n",
    "        z, mu, logvar = self.encoder(src)\n",
    "\n",
    "        hidden = z.unsqueeze(0)\n",
    "        trg_input = trg[:, 0].unsqueeze(1)\n",
    "\n",
    "        for t in range(1, seq_len):\n",
    "            out, hidden = self.decoder(trg_input, hidden)\n",
    "            # Shape of out is (batch_size, 1, trg_vocab_size)\n",
    "            output_seq[:, t] = out.squeeze(1)\n",
    "\n",
    "            if teacher_forcing:\n",
    "                p_teacher_forcing = torch.rand(1).item()\n",
    "                if p_teacher_forcing < teacher_forcing_ratio:\n",
    "                    trg_input = trg[:, t].unsqueeze(1)\n",
    "\n",
    "                else:\n",
    "                    trg_input = out.argmax(dim=2)\n",
    "            else:\n",
    "                trg_input = out.argmax(dim=2)\n",
    "\n",
    "        return z, mu, logvar, output_seq, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape is  torch.Size([10, 32]) mu shape is  torch.Size([10, 32]) logvar shape is  torch.Size([10, 32])\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae_x = torch.randint(0, 39, (10, 12), dtype=torch.int64) # 10 samples, 12 sequence length\n",
    "vae_y = torch.randint(0, 39, (10, 12), dtype=torch.int64) # 10 samples, 12 sequence length\n",
    "\n",
    "z, mu, logvar = vae.encoder(vae_x)\n",
    "print('z shape is ', z.shape, 'mu shape is ', mu.shape, 'logvar shape is ', logvar.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The latent dimension is not the same as the hidden dimension, we do the linear transformation\n",
      "After linear transformation, the hidden shape is:  torch.Size([1, 10, 128])\n",
      "Decoder out shape is  torch.Size([10, 12, 39]) hidden shape is  torch.Size([1, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "decoder_out, hidden = vae.decoder(vae_y, z.unsqueeze(0))\n",
    "print('Decoder out shape is ', decoder_out.shape, 'hidden shape is ', hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 39])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_out = decoder_out[:, 4, :].unsqueeze(1)\n",
    "decoder_out.shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The latent dimension is not the same as the hidden dimension, we do the linear transformation\n",
      "After linear transformation, the hidden shape is:  torch.Size([1, 10, 128])\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "z shape is  torch.Size([10, 32]) mu shape is  torch.Size([10, 32]) logvar shape is  torch.Size([10, 32])\n",
      "Output seq shape is  torch.Size([10, 12, 39]) hidden shape is  torch.Size([1, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "z, mu, logvar, output_seq, hidden = vae(vae_x, vae_y)\n",
    "print('z shape is ', z.shape, 'mu shape is ', mu.shape, 'logvar shape is ', logvar.shape)\n",
    "print('Output seq shape is ', output_seq.shape, 'hidden shape is ', hidden.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000],\n",
       "        [0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000],\n",
       "        [0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000],\n",
       "        [0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000],\n",
       "        [0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000],\n",
       "        [0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000],\n",
       "        [0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000],\n",
       "        [0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000],\n",
       "        [0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000],\n",
       "        [0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq.sum(dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN demo,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 100, 128]), torch.Size([1, 10, 128]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(input_size=64, hidden_size=128, num_layers=1, batch_first=True)\n",
    "x = torch.randn(10, 100, 64) # Batch size 10, sequence length 12, input size 64\n",
    "hidden = torch.randn(1, 10, 128) # num_layers 1, batch size 10, hidden size 128\n",
    "out, hidden = rnn(x, hidden)\n",
    "out.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import from the generative.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape is  torch.Size([10, 32]) mu shape is  torch.Size([10, 32]) logvar shape is  torch.Size([10, 32])\n",
      "Warning: The latent dimension is not the same as the hidden dimension, we do the linear transformation\n",
      "After linear transformation, the hidden shape is:  torch.Size([1, 10, 128])\n",
      "Decoder out shape is  torch.Size([10, 12, 39]) hidden shape is  torch.Size([1, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "from generative_model import VAE_Encoder, VAE_Decoder, VAE\n",
    "import torch\n",
    "\n",
    "encoder = VAE_Encoder(word_count=39, d_model=64, latent_dim=32)\n",
    "decoder = VAE_Decoder(latent_dim=32, word_count=39, d_model=64, hidden_dim=128, output_dim=39, num_layers=1)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "vae_x = torch.randint(0, 39, (10, 12), dtype=torch.int64) # 10 samples, 12 sequence length\n",
    "vae_y = torch.randint(0, 39, (10, 12), dtype=torch.int64) # 10 samples, 12 sequence length\n",
    "\n",
    "z, mu, logvar = vae.encoder(vae_x)\n",
    "print('z shape is ', z.shape, 'mu shape is ', mu.shape, 'logvar shape is ', logvar.shape)\n",
    "\n",
    "decoder_out, hidden = vae.decoder(vae_y, z.unsqueeze(0))\n",
    "print('Decoder out shape is ', decoder_out.shape, 'hidden shape is ', hidden.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The latent dimension is not the same as the hidden dimension, we do the linear transformation\n",
      "After linear transformation, the hidden shape is:  torch.Size([1, 10, 128])\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "The hidden satisfies the requirement\n",
      "z shape is  torch.Size([10, 32]) mu shape is  torch.Size([10, 32]) logvar shape is  torch.Size([10, 32])\n",
      "Output seq shape is  torch.Size([10, 12, 39]) hidden shape is  torch.Size([1, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "z, mu, logvar, output_seq, hidden = vae(vae_x, vae_y)\n",
    "print('z shape is ', z.shape, 'mu shape is ', mu.shape, 'logvar shape is ', logvar.shape)\n",
    "print('Output seq shape is ', output_seq.shape, 'hidden shape is ', hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 35, 26, 18, 22,  6,  0, 14, 38, 21, 38,  6],\n",
       "        [ 0, 11, 38,  6,  3,  7, 24, 12, 22,  6,  0, 14],\n",
       "        [ 0, 22, 12, 22,  6,  0, 14, 38, 34, 12, 32,  6],\n",
       "        [ 0,  5,  0, 10, 38, 34, 14, 26, 22, 12, 22,  6],\n",
       "        [ 0, 27,  1,  1, 16, 14,  6,  3,  4, 19, 19, 12],\n",
       "        [ 0, 17, 38,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "        [ 0, 22,  6,  0, 14, 38, 34, 12, 32,  6,  0, 14],\n",
       "        [ 0, 10, 38, 34, 14, 26, 22, 12, 22,  6,  0, 14],\n",
       "        [ 0,  6,  3,  4, 19, 20,  5,  0,  5, 35,  5, 10],\n",
       "        [ 0, 22,  6,  0, 14, 38, 21, 38,  6,  3,  7, 24]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Process the data debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0\n",
      "data is tensor([[ 0,  1,  7,  1,  1,  1,  0, 12, 13, 13, 13, 13, 13, 13],\n",
      "        [ 0,  1,  2,  1,  1,  1,  1,  1,  1,  1,  0,  3,  1, 12]])\n",
      "label is tensor([[11,  0,  1,  7,  1,  1,  1,  0, 12, 13, 13, 13, 13, 13, 13],\n",
      "        [11,  0,  1,  2,  1,  1,  1,  1,  1,  1,  1,  0,  3,  1, 12]])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.word_counts = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        word = str(word)\n",
    "        if word not in self.word2index:\n",
    "            index = len(self.word2index)\n",
    "            self.word2index[word] = index\n",
    "            self.index2word[index] = word\n",
    "            self.word_counts += 1\n",
    "        return self.word2index[word]  # Return the index directly\n",
    "\n",
    "    def create_vocabulary(self, text):\n",
    "        for sentence in text:\n",
    "            for word in sentence:\n",
    "                self.add_word(word)\n",
    "\n",
    "        for token in ['<start>', '<end>', '<pad>']:\n",
    "            self.add_word(token)\n",
    "\n",
    "        return self.word2index, self.index2word, self.word_counts\n",
    "    \n",
    "    def get_index(self, word):\n",
    "        return self.word2index.get(word, None)  # Use .get() to avoid KeyError\n",
    "    \n",
    "    def get_word(self, index):\n",
    "        return self.index2word.get(index, None)  # Use .get() for safety\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index2word)\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def pad_sequences(self, text_index):\n",
    "        max_length = max(len(sentence) for sentence in text_index)\n",
    "        return [sentence + [self.vocab.get_index('<pad>')] * (max_length - len(sentence)) for sentence in text_index]\n",
    "\n",
    "    def text_to_index(self, text, padding=False, start=False, end=False):\n",
    "        text_index = []\n",
    "        for sentence in text:\n",
    "            sentence_index = []\n",
    "            if start:\n",
    "                sentence_index.append(self.vocab.get_index('<start>'))\n",
    "            sentence_index.extend(self.vocab.get_index(word) for word in sentence)\n",
    "            if end:\n",
    "                sentence_index.append(self.vocab.get_index('<end>'))\n",
    "\n",
    "            text_index.append(torch.tensor(sentence_index, dtype=torch.int64))\n",
    "\n",
    "        if padding:\n",
    "            text_index = self.pad_sequences(text_index)\n",
    "\n",
    "        return text_index\n",
    "\n",
    "class RadonpyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "class RadonpyDataLoader:\n",
    "    def __init__(self, x, y, batch_size, shuffle, vocab):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x, y = zip(*batch)\n",
    "        x = pad_sequence(x, batch_first=True, padding_value=self.vocab.get_index('<pad>'))\n",
    "        y = pad_sequence(y, batch_first=True, padding_value=self.vocab.get_index('<pad>'))\n",
    "        return x, y\n",
    "        \n",
    "    def get_dataloader(self):\n",
    "        dataset = RadonpyDataset(self.x, self.y)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=self.shuffle, collate_fn=self.collate_fn)\n",
    "\n",
    "# Load and preprocess data\n",
    "radonpy_data = pd.read_csv('./radonpy.csv')\n",
    "radonpy_data.columns = radonpy_data.columns.str.replace(' ', '')\n",
    "\n",
    "smiles_list = radonpy_data['smiles'].tolist()[:100]\n",
    "\n",
    "smiles_vocabulary = Vocabulary()\n",
    "smiles_vocabulary.create_vocabulary(smiles_list)\n",
    "\n",
    "smiles_processor = TextPreprocessor(smiles_vocabulary)\n",
    "x = smiles_processor.text_to_index(smiles_list, start=False, end=True, padding=False)\n",
    "y = smiles_processor.text_to_index(smiles_list, start=True, end=True, padding=False)\n",
    "\n",
    "dataloader = RadonpyDataLoader(x, y, batch_size=2, shuffle=True, vocab=smiles_vocabulary).get_dataloader()\n",
    "\n",
    "for i, (data,label) in enumerate(dataloader):\n",
    "    print('Batch ', i)\n",
    "    print('data is', data)\n",
    "    print('label is', label)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Sentences (Data): [['word92', 'word59', 'word47', 'word85', 'word55', 'word72', 'word83', 'word75', 'word29', 'word65', '<PAD>', '<PAD>'], ['word97', 'word48', 'word21', 'word8', 'word99', 'word52', 'word20', 'word7', 'word23', 'word66', 'word46', 'word89']]\n",
      "Batch Labels: [['word92', 'word59', 'word47', 'word85', 'word55', 'word72', 'word83', 'word75', 'word29', 'word65', 'label_extra_word', '<PAD>', '<PAD>'], ['word97', 'word48', 'word21', 'word8', 'word99', 'word52', 'word20', 'word7', 'word23', 'word66', 'word46', 'word89', 'label_extra_word']]\n",
      "--------------------------------------------------\n",
      "Batch Sentences (Data): [['word37', 'word55', 'word100', 'word67', 'word48', 'word50', 'word77', 'word18', 'word88', 'word33', 'word61', 'word78', 'word76', 'word86'], ['word56', 'word77', 'word27', 'word48', 'word36', 'word55', 'word13', 'word60', 'word12', 'word92', 'word91', 'word78', '<PAD>', '<PAD>']]\n",
      "Batch Labels: [['word37', 'word55', 'word100', 'word67', 'word48', 'word50', 'word77', 'word18', 'word88', 'word33', 'word61', 'word78', 'word76', 'word86', 'label_extra_word'], ['word56', 'word77', 'word27', 'word48', 'word36', 'word55', 'word13', 'word60', 'word12', 'word92', 'word91', 'word78', 'label_extra_word', '<PAD>', '<PAD>']]\n",
      "--------------------------------------------------\n",
      "Batch Sentences (Data): [['word67', 'word26', 'word41', 'word12', 'word64', 'word7'], ['word55', 'word56', 'word46', 'word75', 'word66', '<PAD>']]\n",
      "Batch Labels: [['word67', 'word26', 'word41', 'word12', 'word64', 'word7', 'label_extra_word'], ['word55', 'word56', 'word46', 'word75', 'word66', 'label_extra_word', '<PAD>']]\n",
      "--------------------------------------------------\n",
      "Batch Sentences (Data): [['word2', 'word59', 'word39', 'word26', 'word36', 'word18', 'word22', '<PAD>'], ['word11', 'word85', 'word67', 'word78', 'word77', 'word59', 'word91', 'word94']]\n",
      "Batch Labels: [['word2', 'word59', 'word39', 'word26', 'word36', 'word18', 'word22', 'label_extra_word', '<PAD>'], ['word11', 'word85', 'word67', 'word78', 'word77', 'word59', 'word91', 'word94', 'label_extra_word']]\n",
      "--------------------------------------------------\n",
      "Batch Sentences (Data): [['word43', 'word56', 'word17', 'word57', 'word44', 'word61', 'word9', 'word60', 'word12', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['word47', 'word80', 'word93', 'word30', 'word32', 'word84', 'word79', 'word16', 'word36', 'word88', 'word45', 'word97', 'word77', 'word19', 'word82']]\n",
      "Batch Labels: [['word43', 'word56', 'word17', 'word57', 'word44', 'word61', 'word9', 'word60', 'word12', 'label_extra_word', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['word47', 'word80', 'word93', 'word30', 'word32', 'word84', 'word79', 'word16', 'word36', 'word88', 'word45', 'word97', 'word77', 'word19', 'word82', 'label_extra_word']]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# A simple function to generate a random sentence with length between 5 and 15 words\n",
    "def generate_random_sentence(min_len=5, max_len=15):\n",
    "    words = [\"word{}\".format(i) for i in range(1, 101)]  # A list of possible words: ['word1', 'word2', ..., 'word100']\n",
    "    sentence_length = random.randint(min_len, max_len)  # Randomly select sentence length between 5 and 15\n",
    "    return random.sample(words, sentence_length)  # Randomly select words based on the generated length\n",
    "\n",
    "# Custom dataset class for sentences and labels\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sentence, label\n",
    "\n",
    "# Create dummy sentences as data and labels that are one word longer than the data\n",
    "data = [generate_random_sentence() for _ in range(10)]  # 10 sentences, each with random lengths (5 to 15 words)\n",
    "labels = [sentence + ['label_extra_word'] for sentence in data]  # Label is one word longer than the sentence\n",
    "\n",
    "# Function to pad sentences or labels to the maximum length in a batch\n",
    "def pad_sequence(sequence, max_length, pad_token='<PAD>'):\n",
    "    return sequence + [pad_token] * (max_length - len(sequence))  # Pads with <PAD> token\n",
    "\n",
    "# Custom collate function to handle variable-length sentence data\n",
    "def custom_collate_fn(batch):\n",
    "    # `batch` is a list of tuples where each tuple is (sentence, label)\n",
    "    sentences, labels = zip(*batch)\n",
    "\n",
    "    # Find the maximum length of sentences and labels in the current batch\n",
    "    max_sentence_length = max(len(sentence) for sentence in sentences)\n",
    "    max_label_length = max(len(label) for label in labels)\n",
    "\n",
    "    # Pad sentences and labels to the maximum lengths in the batch\n",
    "    padded_sentences = [pad_sequence(sentence, max_sentence_length) for sentence in sentences]\n",
    "    padded_labels = [pad_sequence(label, max_label_length) for label in labels]\n",
    "\n",
    "    return padded_sentences, padded_labels\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = SentenceDataset(data, labels)\n",
    "\n",
    "# Create a DataLoader using the custom collate function\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch_sentences, batch_labels in dataloader:\n",
    "    print(\"Batch Sentences (Data):\", batch_sentences)\n",
    "    print(\"Batch Labels:\", batch_labels)\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for Training debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from typing import Union\n",
    "import os\n",
    "\n",
    "\n",
    "class CalculateMetrics:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "\n",
    "    def accuracy(self):\n",
    "        return accuracy_score(self.y_true, self.y_pred)\n",
    "    \n",
    "    def precision(self):\n",
    "        return precision_score(self.y_true, self.y_pred)\n",
    "    \n",
    "    def recall(self):\n",
    "        return recall_score(self.y_true, self.y_pred)\n",
    "    \n",
    "    def f1(self):\n",
    "        return f1_score(self.y_true, self.y_pred)\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        return [self.accuracy(), self.precision(), self.recall(), self.f1()]\n",
    "\n",
    "\n",
    "\n",
    "class Training:\n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 model : nn.Module = None,\n",
    "                 optimizer : optim = None,\n",
    "                 loss_function : nn.Module = None,\n",
    "                 epochs : int = 10,\n",
    "                 lr_scheduler = None,\n",
    "                 clip = None,\n",
    "                 check_point_path : str = None,\n",
    "                 device = 'cpu'):\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.epochs = epochs\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.clip = clip\n",
    "        self.device = device\n",
    "\n",
    "        if check_point_path:\n",
    "            self.check_point_path = check_point_path\n",
    "        else:\n",
    "            self.check_point_path = os.getcwd() + '/checkpoints'\n",
    "\n",
    "    def save_checkpoint(self, idx):\n",
    "        if not os.path.exists(self.check_point_path):\n",
    "            os.makedirs(self.check_point_path)\n",
    "\n",
    "        torch.save(self.model.state_dict(), self.check_point_path + f'/checkpoint_{idx}.pth')\n",
    "\n",
    "        return self.check_point_path + f'/checkpoint_{idx}.pth'\n",
    "\n",
    "    def _get_index_checkpoint(self, checkpoint_path, checkpoint_name='checkpoint'):\n",
    "        index_list = []\n",
    "        for file in os.listdir(checkpoint_path):\n",
    "            if file.startswith(checkpoint_name):\n",
    "                index_list.append(int(file.split('_')[-1].split('.')[0]))\n",
    "        return index_list\n",
    "\n",
    "    def load_checkpoint(self, \n",
    "                        checkpoint: Union[bool, int] = False):\n",
    "        \n",
    "        if checkpoint:\n",
    "            if isinstance(checkpoint, bool):\n",
    "                # Load the last checkpoint\n",
    "                index_list = self._get_index_checkpoint(self.check_point_path)\n",
    "                idx = max(index_list)\n",
    "                self.model.load_state_dict(torch.load(self.check_point_path + f'/checkpoint_{idx}.pth'))\n",
    "                checkpoint_path = self.check_point_path + f'/checkpoint_{idx}.pth'\n",
    "                return checkpoint_path\n",
    "            \n",
    "            elif isinstance(checkpoint, int):\n",
    "                self.model.load_state_dict(torch.load(self.check_point_path + f'/checkpoint_{checkpoint}.pth'))\n",
    "                checkpoint_path = self.check_point_path + f'/checkpoint_{checkpoint}.pth'\n",
    "                return checkpoint_path\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"checkpoint must be a boolean or an integer\")\n",
    "                   \n",
    "\n",
    "    def train_loop(self, \n",
    "                   train_loader : DataLoader, \n",
    "                   teacher_forcing : bool = False):\n",
    "        # Only do the task to predict one sentence !!!\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for seq_input, seq_output in train_loader:\n",
    "            seq_input, seq_output = seq_input.to(self.device), seq_output.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(seq_input, seq_output)\n",
    "\n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    def test_loop(self, test_loader):\n",
    "        # Same with above\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                total_loss += self.loss_function(output, target).item()\n",
    "\n",
    "        return total_loss / len(test_loader)\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_loss_mask(self, data, mask_value=0):\n",
    "        return torch.where(data != mask_value, torch.tensor(1).to(self.device), torch.tensor(0).to(self.device))\n",
    "    \n",
    "    def fit(self, train_loader, test_loader,\n",
    "                checkpoint_save: Union[bool, int] = False,\n",
    "                checkpoint_load: Union[bool, str] = False,\n",
    "                epochs : int = None):\n",
    "        # Get the dataloader here\n",
    "\n",
    "        \"\"\"\n",
    "        checkpoint: bool or int\n",
    "            If False, no checkpoint will be saved. If True, checkpoint will be saved after each epoch.\n",
    "            If int, checkpoint will be saved after each epoch modulo the int value.\n",
    "        \n",
    "        checkpoint_load: bool or int   \n",
    "            If False, no checkpoint will be loaded. If True, the last checkpoint will be loaded.\n",
    "            If int, the checkpoint will be loaded the epoch modulo the int value.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "\n",
    "        if checkpoint_load:\n",
    "            checkpoint_path = self.load_checkpoint(checkpoint_load)\n",
    "            state_dict = torch.load(checkpoint_path)\n",
    "            self.model.load_state_dict(state_dict)\n",
    "            print(f'Now You Are Training From Checkpoint {checkpoint_path}')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self.train_loop(train_loader)\n",
    "            test_loss = self.test_loop(test_loader)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.test_losses.append(test_loss)\n",
    "\n",
    "            if checkpoint_save:\n",
    "                if isinstance(checkpoint_save, bool):\n",
    "                    self.save_checkpoint(epoch)\n",
    "                    print(f\"Each epoch checkpoint saved at {self.check_point_path}\")\n",
    "                elif isinstance(checkpoint_save, int) and isinstance(checkpoint_load, bool):\n",
    "                    print(\"No checkpoint loaded, so the checkpoint will be saved at each epoch modulo the int value\")\n",
    "                    if epoch % checkpoint_save == 0:\n",
    "                        self.save_checkpoint(epoch)\n",
    "                elif isinstance(checkpoint_save, int) and isinstance(checkpoint_load, int):\n",
    "                    print(f'{checkpoint_path} is loaded, so the checkpoint will be saved at each epoch modulo the int value')\n",
    "                    if epoch % checkpoint_save == 0:\n",
    "                        self.save_checkpoint(epoch + checkpoint_load)\n",
    "        \n",
    "        return self.train_losses, self.test_losses\n",
    "\n",
    "    def predict(self, data):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            data = data.to(self.device)\n",
    "            output = self.model(data)\n",
    "            return output\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         0., 1., 1.],\n",
       "        [0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 0.],\n",
       "        [0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 0., 1.],\n",
       "        [1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1.],\n",
       "        [0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "         0., 0., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
       "         1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "         1., 1., 1.]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='none')  # Keep individual losses\n",
    "\n",
    "predictions = torch.randn(10, 12, 39)  # (batch_size, seq_len, vocab_size)\n",
    "targets = torch.randint(0, 5, (10, 39))  # (batch_size, seq_len)\n",
    "\n",
    "# Compute loss (batch_size, seq_len)\n",
    "loss = loss_fn(predictions, targets)\n",
    "\n",
    "# Create a mask to ignore <PAD> tokens (assuming <PAD> is token index 0)\n",
    "mask = (targets != 0).float()\n",
    "\n",
    "# Apply the mask to the loss\n",
    "loss = loss * mask\n",
    "\n",
    "# Optionally, sum the losses over valid tokens and take the average\n",
    "loss = loss.sum() / mask.sum()  # Averaging only over non-padded tokens\n",
    "mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chatgpt demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 12, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)  # Embedding layer\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True)  # RNN layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_len, emb_dim)\n",
    "        outputs, hidden = self.rnn(embedded)  # outputs: (batch_size, seq_len, hidden_dim), hidden: (num_layers, batch_size, hidden_dim)\n",
    "        return hidden  # We return the final hidden state to initialize the decoder\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)  # Embedding layer\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True)  # RNN layer\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)  # Output layer to project RNN hidden state to output tokens\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x: (batch_size) contains the index of the current input token\n",
    "        x = x.unsqueeze(1)  # Add the time dimension: (batch_size) -> (batch_size, 1)\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, 1, emb_dim)\n",
    "        output, hidden = self.rnn(embedded, hidden)  # output: (batch_size, 1, hidden_dim), hidden: (num_layers, batch_size, hidden_dim)\n",
    "        prediction = self.fc_out(output)  # Shape: (batch_size, 1, output_dim)\n",
    "        prediction = self.softmax(prediction)  # Apply softmax to get probabilities over the vocabulary\n",
    "        return prediction.squeeze(1), hidden  # Remove the time dimension for prediction\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        src: source sentence (batch_size, src_len)\n",
    "        trg: target sentence (batch_size, trg_len)\n",
    "        teacher_forcing_ratio: the probability of using teacher forcing\n",
    "        \"\"\"\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        # Initialize an empty tensor to store the decoder's predictions\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # Encode the source sequence\n",
    "        hidden = self.encoder(src)  # Shape: (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "        # First input to the decoder is the <START> token (usually tokenized as 0)\n",
    "        input = trg[:, 0]  # Shape: (batch_size)\n",
    "\n",
    "        # Loop through the target sequence length\n",
    "        for t in range(1, trg_len):\n",
    "            # Get the prediction from the decoder\n",
    "            output, hidden = self.decoder(input, hidden)  # output: (batch_size, output_dim)\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            # Determine if we will use teacher forcing or not\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)  # Get the predicted token with the highest probability\n",
    "\n",
    "            # If teacher forcing, use the actual next token as input; else, use the predicted token\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Hyperparameters\n",
    "INPUT_DIM = 1000  # Vocabulary size of the input\n",
    "OUTPUT_DIM = 1000  # Vocabulary size of the output (should match target vocab size)\n",
    "EMB_DIM = 256  # Embedding size for both encoder and decoder\n",
    "HIDDEN_DIM = 512  # Hidden state dimension\n",
    "NUM_LAYERS = 1  # Number of layers in the RNN\n",
    "BATCH_SIZE = 32\n",
    "SRC_SEQ_LEN = 10  # Source sentence length\n",
    "TRG_SEQ_LEN = 12  # Target sentence length\n",
    "\n",
    "# Define encoder and decoder\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, NUM_LAYERS)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, NUM_LAYERS)\n",
    "\n",
    "# Create Seq2Seq model\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Example batch of source sentences (batch_size, src_seq_len) and target sentences (batch_size, trg_seq_len)\n",
    "src = torch.randint(0, INPUT_DIM, (BATCH_SIZE, SRC_SEQ_LEN)).to(device)\n",
    "trg = torch.randint(0, OUTPUT_DIM, (BATCH_SIZE, TRG_SEQ_LEN)).to(device)\n",
    "\n",
    "# Forward pass (with teacher forcing)\n",
    "outputs = model(src, trg, teacher_forcing_ratio=0.75)  # Shape: (batch_size, trg_seq_len, output_dim)\n",
    "print(outputs.shape)  # (32, 12, 1000) - Batch size, target sequence length, vocabulary size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at first trg shape: torch.Size([32, 12])\n",
      "output shape: torch.Size([32, 12, 1000])\n",
      "After reshape output shape: torch.Size([352, 1000])\n",
      "After reshape trg shape: torch.Size([352])\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Example batch of target sentences (ground truth)\n",
    "# trg has shape (batch_size, trg_len) where each value is the index of the token\n",
    "trg = torch.randint(0, OUTPUT_DIM, (BATCH_SIZE, TRG_SEQ_LEN)).to(device)\n",
    "\n",
    "print('at first trg shape:', trg.shape)\n",
    "# Forward pass through the model\n",
    "outputs = model(src, trg, teacher_forcing_ratio=0.75)  # Shape: (batch_size, trg_len, output_dim)\n",
    "print('output shape:', outputs.shape)\n",
    "\n",
    "# Reshape the outputs and target to match the requirements of nn.CrossEntropyLoss\n",
    "# The loss function expects inputs of shape (batch_size * trg_len, vocab_size)\n",
    "# and target of shape (batch_size * trg_len)\n",
    "outputs = outputs[:, 1:].reshape(-1, OUTPUT_DIM)  # Skip <START> token and flatten to (batch_size * trg_len, output_dim)\n",
    "print('After reshape output shape:', outputs.shape)\n",
    "trg = trg[:, 1:].reshape(-1)  # Skip <START> token and flatten to (batch_size * trg_len)\n",
    "print('After reshape trg shape:', trg.shape)\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(outputs, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
